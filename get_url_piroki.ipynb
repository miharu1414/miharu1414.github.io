{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "load_url = \"https://www.ski-gelende.com/gelende/\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "html = str(soup)\n",
    "urls = []\n",
    "names = []\n",
    "cnt = 3\n",
    "while True:\n",
    "    try:\n",
    "        url = re.search( r'(?<=<li><a href=\").+?(?=\">)', html).group(0)\n",
    "        urls.append(url)\n",
    "        \n",
    "        index = html.find(\"<li><a href=\\\"\"+url+\"\\\">\")\n",
    "        html = html[index:]\n",
    "        html = html.replace('<li><a href=\\\"'+url+'\\\"','')\n",
    "        \n",
    "        name = re.search( r'(?<=>).+?(?=</a>)', html).group(0)\n",
    "        names.append(name)\n",
    "        if name == \"箱館山スキー場\":\n",
    "            break\n",
    "    except:\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "url_list = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "  # Google検索するキーワードを設定\n",
    "  search_word = names[i]\n",
    "\n",
    "  # 上位から何件までのサイトを抽出するか指定する\n",
    "  pages_num = 1 + 1\n",
    "\n",
    "  print(f'【検索ワード】{search_word}')\n",
    "\n",
    "  # Googleから検索結果ページを取得する\n",
    "  url = f'https://www.google.co.jp/search?hl=ja&num={pages_num}&q={search_word}'\n",
    "  request = requests.get(url)\n",
    "\n",
    "  # Googleのページ解析を行う\n",
    "  soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "  search_site_list = soup.select('div.kCrYT > a')\n",
    "\n",
    "  # ページ解析と結果の出力\n",
    "  for rank, site in zip(range(1, pages_num), search_site_list):\n",
    "      try:\n",
    "          site_title = site.select('h3.zBAuLc')[0].text\n",
    "      except IndexError:\n",
    "          site_title = site.select('img')[0]['alt']\n",
    "      site_url = site['href'].replace('/url?q=', '')\n",
    "      # 結果を出力する\n",
    "      target1 = 'http'\n",
    "      idx1 = site_url.find(target1)\n",
    "      target2 = '&sa=U'\n",
    "      idx2 = site_url.find(target2)\n",
    "      site_url = site_url[idx1 + len(target1):idx2]\n",
    "      site_url = 'http' + site_url\n",
    "      url_list.append(site_url)\n",
    "\n",
    "print(url_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
