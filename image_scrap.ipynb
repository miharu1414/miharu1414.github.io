{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install icrawler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "features_name = [\"スキー場名\",\"url\",\"リフト数\",\"コース数\",\"トップ標高\",\"最大斜度\",\"標高差\",\"最長滑走距離\",\"初級\",\"中級\",\"上級\",\"圧雪\",\"非圧雪\",\"コブ\",\"スキーヤー\",\"ボーダー\"]\n",
    "data = []\n",
    "error_cnt = 0\n",
    "errors =[]\n",
    "for i in range(len(names)):\n",
    "    load_url = urls[i]\n",
    "    html = requests.get(load_url)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        html = str(soup)\n",
    "        #print(names[i])\n",
    "        #print(html)\n",
    "        #print(i,names[i])\n",
    "        \n",
    "        features = []\n",
    "        features.append(names[i])\n",
    "        name = names[i]\n",
    "        features.append(load_url)\n",
    "        \n",
    "        lift = re.search( r'(?<=<p class=\"p76\">リフト数</p><p class=\"h2\">).+?(?=</p>)', html).group(0)\n",
    "\n",
    "        lift = re.search( r'(?<=コース数</p><p class=\"h2\">).+?(?=</p>)', html).group(0)\n",
    "        \n",
    "        height = re.search( r'(?<=トップ標高：</dt><dd class=\"col-sm-7 col-xs-6 p5\">).+?(?=</dd>)', html).group(0)\n",
    "        if \"m\" not in height:\n",
    "            height = height + 'm'\n",
    "        \n",
    "        lean = re.search( r'(?<=最大斜度：</dt><dd class=\"col-sm-7 col-xs-6 p5\">).+?(?=</dd>)', html).group(0)\n",
    "        \n",
    "        dif_height = re.search( r'(?<=標高差：</dt><dd class=\"col-sm-7 col-xs-6 p5\">).+?(?=</dd>)', html).group(0)\n",
    "        if \"標高差：\" in dif_height:\n",
    "            dif_height = dif_height.replace(\"標高差：\",\"\")\n",
    "        \n",
    "        distant = re.search( r'(?<=最長滑走距離：</dt><dd class=\"col-sm-7 col-xs-6 p5\">).+?(?=</dd>)', html).group(0)\n",
    "        \n",
    "        try:\n",
    "            biginner = re.search( r'(?<=初級<br/>).+?(?=</td>)', html).group(0)\n",
    "        except:\n",
    "            biginner = re.search( r'(?<=初級<br>).+?(?=</td>)', html).group(0)\n",
    "        if \"</br>\" in biginner:\n",
    "            biginner = biginner.replace(\"</br>\",\"\")\n",
    "        \n",
    "        middle = re.search( r'(?<=中級<br/>).+?(?=</td>)', html).group(0)\n",
    "        \n",
    "        expert = re.search( r'(?<=上級<br/>).+?(?=</td>)', html).group(0)\n",
    "        \n",
    "        try:\n",
    "            pist = re.search( r'(?<=圧雪<br/>).+?(?=</td>)', html).group(0)\n",
    "        except:\n",
    "            pist = '0%'\n",
    "        \n",
    "        try:\n",
    "            powd = re.search( r'(?<=非圧雪<br/>).+?(?=</td>)', html).group(0)\n",
    "        except:\n",
    "            powd = '0%'\n",
    "        \n",
    "        try:\n",
    "            hump = re.search( r'(?<=コブ<br/>).+?(?=</td>)', html).group(0)\n",
    "            \n",
    "        except:\n",
    "            hump = '0%'\n",
    "        \n",
    "        skier = re.search( r'(?<=スキーヤー<br/>).+?(?=</td>)', html).group(0)\n",
    "        if \"</p>\" in skier:\n",
    "            skier = skier.replace(\"</p>\",\"\")\n",
    "\n",
    "        \n",
    "        try:\n",
    "            border = re.search( r'(?<=ボーダー<br/>).+?(?=</td>)', html).group(0)\n",
    "            \n",
    "        except:\n",
    "            border  = '0%'\n",
    "        features.append(border)\n",
    "        \n",
    "        data.append(features)\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        error_cnt += 1\n",
    "        errors.append(names[i])\n",
    "    \n",
    "print(features)\n",
    "\n",
    "\n",
    "# expart = html.find(\"<li><a href=\\\"\"+url+\"\\\">\")\n",
    "# html = html[index:]\n",
    "\n",
    "# index = html.find(\"<li><a href=\\\"\"+url+\"\\\">\")\n",
    "# html = html[index:]\n",
    "# html = html.replace('<li><a href=\\\"'+url+'\\\"','')\n",
    "\n",
    "# name = re.search( r'(?<=>).+?(?=</a>)', html).group(0)\n",
    "# names.append(name)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icrawler.builtin import BingImageCrawler\n",
    "\n",
    "for i in range(len(data)):\n",
    "  # クローラーを生成、保存先などを指定（今回は'images'フォルダに指定）\n",
    "  bing_crawler = BingImageCrawler(downloader_threads=4,storage={'root_dir': str(i)})\n",
    "\n",
    "  # キーワードや枚数を入力させてそれに応じて画像収集する \n",
    "  bing_crawler.crawl(keyword=data[i][0], filters=None, offset=0, max_num=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_name1 = []\n",
    "html_name2 = \"</div>\"\n",
    "html_url1 = []\n",
    "html_url2 = \"</a>\"\n",
    "html_place_name = []\n",
    "html_images = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "  html_name1.append('<div class=\"result_contents\" key=\"' + str(i) + '\" v-if=\"result_num==' + str(i) + '\">')\n",
    "  html_url1.append('<a href=\"' + url_list[i] + '\">') \n",
    "  html_place_name.append(data[i][0])\n",
    "  html_images.append('<img src = \"../image_new/' + str(i) + '/000001.jpg\" class=\"image-vw\">')\n",
    "  \n",
    "for i in range(len(data)):\n",
    "  print(html_name1[i])\n",
    "  print(html_url1[i])\n",
    "  print(\"<label>\")\n",
    "  print(html_images[i])\n",
    "  print(\"<hr>\")\n",
    "  print(html_place_name[i])\n",
    "  print(\"</label>\")\n",
    "  print(html_url2)\n",
    "  print(html_name2)\n",
    "\n",
    "print(html_name1)\n",
    "print(html_url1)\n",
    "print(html_place_name)\n",
    "print(html_images)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
